참고사이트 : https://m.blog.naver.com/PostView.nhn?blogId=magnking&logNo=221311273459&proxyReferer=https%3A%2F%2Fwww.google.com%2F

FFNets (feed-forward neural networks)
	일반적인 신경망
	데이터를 트레이닝 셋과 테스트 셋으로 나누어 관리하고 트레이닝 셋을 통해서 신경망의 가중치를 학습시키고 이 결과를 테스트셋을 통해서 확인하는 방식
	데이터를 입력하면 입력층에서 은닉층까지 연산이 차근차근 진행되고 출력이 나가게 된다.
		이 과정에서 입력 데이터는 모든 노드를 딱 한번씩 지나가게 된다.
			이말은 데이터의 순서 (시간적인 측면)을 고려하지 않는 구조라는 의미이다.

RNN (Recurrent Neural Networks)
	RNN은 FFNets와 다르게 은닉층의 결과가 다시 같은 은닉층의 입력으로 들어가도록 연결되어 있다.
		순서 또는 시간이라는 측면을 고려할 수 있는 특징을 갖게 된다.
	따라서 RNN은 순서적인 측면을 고려해서 판단할 수 있으므로 sequence data를 다루는데 도움이 된다.
		sequence data - ex) 문장, 유전자, 손글씨, 음성신호, 주가
		문장에 있는 단어의 뜻을 살필때 현재의 단어만으로 의미를 해석하는 것이 아닌 앞 단어와의 관계를 통해서 현재 단어의 의미를 해석할 수 있다.

	한계
	Long-Term dependeny
		제공된 데이터와 참고해야할 정보의 입력 위치 차이가 커지면 문제가 생긴다
			ex ) the clouds are in the sky
				주어진 문장 : the clouds are in the
				예측해야할 단어 :  sky
					=> 문제 없음

				I grew up in France ... I speak fluent French
				주어진 문장 : I grew up in France ... I speak fluent 
				예측해야할 단어 : French
				French를 얘측하는데 France가 필요하다
				France와 French의 gap이 꽤 크므로 문제가 생긴다.

	LSTM에서는 이러한 한계를 개선한다.


LSTM (Long short term memory)
	표준 RNN은 싱글레이어였던 것과는 다르게 LSTM은 상호작용을 하는 4개의 레이어가 존재한다.
	LSTM은 RNN과 조금 다르게 은닉층에 다시 정보를 넘겨줄때 출력값만 넘겨주던 RNN과 다르게 출력값과 cell state의 값을 넘겨준다.
	cell state - 하나의 컨베이어 벨트처럼 전체 체인을 통과함. 
		그림에서 다음 은닉층으로 넘겨지는 2개의 선중 위의 선 (아래 선은 출력이 다음 은닉층으로 넘겨지는 선임)
		cell state에 gate라는 요소를 활용해서 정보를 더하거나 제거하는 기능을 수행한다.
			gate - 선택적으로 정보들이 흘러 들어갈 수 있도록 만드는 장치
				시그모이드 신경망 레이어와 점단위 곱하기 연산으로 구성되어 있다.
					시그모이드 레이어 - 0 혹은 1의 값을 출력한다. (0이면 영향x, 1이면 영향 o)
									각 구성요소가 영향을 주게 될지 결정하는 역할
		cell state에서는 보통 3가지 게이트를 활용한다
			1. forget gate layer(시그모이드 레이어)
			2. input gate layer(시그모이드 레이어)
			3. tanh layer





참고사이트 : https://mongxmongx2.tistory.com/25
ReLU (Rectified Linear Unit) - activation function으로 사용된다.
	activation function으로 sigmoid를 많이 사용했지만 gradient vanishing 문제로 인해 relu가 많이 사용되고 있다.
		gradient vanishing - backpropagation 수행 시 layer를 지나면서 gradient를 계속 곱하는데 이 과정에서 gradient가 0으로 수렴하게 됨 (layer가 많아질수록 심해짐)
	relu는 입력값이 0보다 작으면 0이고 0보다 크면 입력값 그대로를 내보낸다.




참고사이트 : https://m.blog.naver.com/wideeyed/221021710286
softmax - 활성화 함수
	입력받은 값을 출력으로 0~1 사이의 값으로 모두 정규화하며 출력 값들의 총합은 항상 1이 되는 특성을 갖는 함수





참고사이트 : https://ganghee-lee.tistory.com/30, https://ganghee-lee.tistory.com/31
ReLu나 sigmoide는 활성화함수이다. 그러면 활성화 함수는 왜 쓰는가 ?

xor과 같이 linear한 방식으로는 분류할 수 없는 일이 생겼고 이를 해결하기 위해 non-linear한 방식으로 출력값을 도출해야 했다.
활성화 함수를 통해 linear system을 non-linear system으로 바꿀 수 있게 되었다.
하지만 활성화 함수를 이용하여 비선형 시스템인 MLP(multiple layer perceptron)을 만들면 MLP의 파라미터 개수가 점점 많아지면서 각각의 weight와 bias를 학습시키는 것이 매우 어려워진다.

이러한 문제를 해결하기 위해서 backpropagation(역전파)라는 방법을 쓰게 된다.
출력값에 대한 입력값의 기울기(미분값)을 출력층 layer에서부터 계산하여 거꾸로 전파시키는 방법이다.
각 layer에서 기울기 값을 구하고 이 기울기 값을 이용하여 gradient descent방법으로 w와 b를 업데이트 시켜 w와 b를 구한다.

relu => 역전파를 통해 좋은 성능이 나오기 때문에 마지막 층이 아니고서야 거의 relu 사용
sigmoid => yes or no 와 같은 이진 분류 문제
softmax => 확률 값을 이용해 다양한 클래스를 분류하기 위한 문제




참고사이트 : https://ganghee-lee.tistory.com/28
loss function과 cost function은 무엇인가 ? 둘의 차이점은 ?
loss function은 1개의 input의 결과인 y값과 실제 y 값 사이의 오차를 계산하는 함수
cost function은 모든 input dataset에 대해서 오차를 계산하는 함수로써 모든 input dataset에 대해 계산한 loss function의 평균값으로 구할 수 있다.















